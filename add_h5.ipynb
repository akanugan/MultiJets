{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT700to1000.h5\n",
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT1000to1500.h5\n",
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT1500to2000.h5\n",
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT2000toInf.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "def is_empty_event(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        event_vars_group = h5_file.get('EventVars')\n",
    "        if event_vars_group and 'normweight' in event_vars_group and len(event_vars_group['normweight']) == 0:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def copy_items(name, obj, destination_group):\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        if name not in destination_group:\n",
    "            new_group = destination_group.create_group(name)\n",
    "            obj.visititems(lambda subname, subobj: copy_items(subname, subobj, new_group))\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "\n",
    "        if name in destination_group:\n",
    "            existing_dataset = destination_group[name]\n",
    "            existing_shape = existing_dataset.shape\n",
    "            existing_dataset.resize((existing_shape[0] + obj.shape[0],) + existing_shape[1:])\n",
    "            existing_dataset[-obj.shape[0]:] = obj[:]\n",
    "        else:\n",
    "            chunks = (1,) + obj.shape[1:]\n",
    "            destination_group.create_dataset(name, shape=obj.shape, dtype=obj.dtype, chunks=chunks, maxshape=(None,) + obj.shape[1:], data=obj[:])\n",
    "\n",
    "def concatenate_h5_in_folder(output_file, folder='unsupervised-search/QCD_HT_h5s'):\n",
    "    files_to_concatenate = glob.glob(os.path.join(folder, '*.h5'))\n",
    "\n",
    "\n",
    "    with h5py.File(output_file, 'w') as output_h5:\n",
    "        for file_path in files_to_concatenate:\n",
    "            print(file_path)\n",
    "            #if not is_empty_event(file_path):\n",
    "            with h5py.File(file_path, 'r') as input_h5:\n",
    "                input_normweight = input_h5.get('EventVars/normweight')\n",
    "                if input_normweight is not None and len(input_normweight) > 0:\n",
    "                    input_h5.visititems(lambda name, obj: copy_items(name, obj, output_h5))\n",
    "\n",
    "output_file_path = 'unsupervised-search/combined_QCD.h5'\n",
    "concatenate_h5_in_folder(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT700to1000.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accessing a group is done with bytes or str, not <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     65\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupervised-search/combined_QCD_400k.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 66\u001b[0m \u001b[43msample_and_concatenate_h5\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36msample_and_concatenate_h5\u001b[0;34m(output_file, folder, desired_total_events)\u001b[0m\n\u001b[1;32m     51\u001b[0m random_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(num_events_in_file, size\u001b[38;5;241m=\u001b[39mnum_events_to_sample, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Copy sampled events to the output file\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#input_h5.visititems(lambda name, obj: copy_items(name, obj[random_indices], output_h5))\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Copy sampled events to the output file\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43minput_h5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisititems\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy_items\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_h5\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m total_events_written \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_events_to_sample\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Check if we have reached the desired total number of events\u001b[39;00m\n",
      "File \u001b[0;32m/uscms_data/d3/akanugan/mambaforge/envs/multijets_env/lib/python3.11/site-packages/h5py/_hl/group.py:668\u001b[0m, in \u001b[0;36mGroup.visititems\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    666\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_d(name)\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(name, \u001b[38;5;28mself\u001b[39m[name])\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:354\u001b[0m, in \u001b[0;36mh5py.h5o.visit\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:301\u001b[0m, in \u001b[0;36mh5py.h5o.cb_obj_simple\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/uscms_data/d3/akanugan/mambaforge/envs/multijets_env/lib/python3.11/site-packages/h5py/_hl/group.py:667\u001b[0m, in \u001b[0;36mGroup.visititems.<locals>.proxy\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Use the text name of the object, not bytes \"\"\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_d(name)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 56\u001b[0m, in \u001b[0;36msample_and_concatenate_h5.<locals>.<lambda>\u001b[0;34m(name, obj)\u001b[0m\n\u001b[1;32m     51\u001b[0m random_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(num_events_in_file, size\u001b[38;5;241m=\u001b[39mnum_events_to_sample, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Copy sampled events to the output file\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#input_h5.visititems(lambda name, obj: copy_items(name, obj[random_indices], output_h5))\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Copy sampled events to the output file\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m input_h5\u001b[38;5;241m.\u001b[39mvisititems(\u001b[38;5;28;01mlambda\u001b[39;00m name, obj: copy_items(\u001b[38;5;28mstr\u001b[39m(name), \u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom_indices\u001b[49m\u001b[43m]\u001b[49m, output_h5))\n\u001b[1;32m     59\u001b[0m total_events_written \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_events_to_sample\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Check if we have reached the desired total number of events\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/uscms_data/d3/akanugan/mambaforge/envs/multijets_env/lib/python3.11/site-packages/h5py/_hl/group.py:359\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m h5o\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e(name), lapl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lapl)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n\u001b[1;32m    362\u001b[0m otype \u001b[38;5;241m=\u001b[39m h5i\u001b[38;5;241m.\u001b[39mget_type(oid)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mGROUP:\n",
      "\u001b[0;31mTypeError\u001b[0m: Accessing a group is done with bytes or str, not <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "def is_empty_event(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        event_vars_group = h5_file.get('EventVars')\n",
    "        if event_vars_group and 'normweight' in event_vars_group and len(event_vars_group['normweight']) == 0:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def copy_items(name, obj, destination_group):\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        if name not in destination_group:\n",
    "            new_group = destination_group.create_group(name)\n",
    "            obj.visititems(lambda subname, subobj: copy_items(subname, subobj, new_group))\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        if name in destination_group:\n",
    "            existing_dataset = destination_group[name]\n",
    "            existing_shape = existing_dataset.shape\n",
    "            existing_dataset.resize((existing_shape[0] + obj.shape[0],) + existing_shape[1:])\n",
    "            existing_dataset[-obj.shape[0]:] = obj[:]\n",
    "        else:\n",
    "            chunks = (1,) + obj.shape[1:]\n",
    "            destination_group.create_dataset(name, shape=obj.shape, dtype=obj.dtype, chunks=chunks, maxshape=(None,) + obj.shape[1:], data=obj[:])\n",
    "\n",
    "def has_pt_above_threshold(pt_dataset, threshold, min_jets):\n",
    "    above_threshold_indices = pt_dataset[:] > threshold\n",
    "    return above_threshold_indices.sum() >= min_jets\n",
    "\n",
    "def concatenate_h5_in_folder(output_file, folder='unsupervised-search/QCD_HT_h5s', threshold=2, min_jets=4):\n",
    "    files_to_concatenate = glob.glob(os.path.join(folder, '*.h5'))\n",
    "\n",
    "    with h5py.File(output_file, 'w') as output_h5:\n",
    "        for file_path in files_to_concatenate:\n",
    "            print(file_path)\n",
    "            with h5py.File(file_path, 'r') as input_h5:\n",
    "                input_pt_dataset = input_h5.get('source/pt')\n",
    "                if input_pt_dataset is not None and has_pt_above_threshold(input_pt_dataset, threshold, min_jets):\n",
    "                    input_h5.visititems(lambda name, obj: copy_items(name, obj, output_h5))\n",
    "\n",
    "output_file_path = 'unsupervised-search/combined_QCD_400k.h5'\n",
    "concatenate_h5_in_folder(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT700to1000.h5\n",
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT1000to1500.h5\n",
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT2000toInf.h5\n",
      "unsupervised-search/QCD_HT_h5s/combined_out_QCD_HT1500to2000.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "def is_empty_event(file_path):\n",
    "    with h5py.File(file_path, 'r') as h5_file:\n",
    "        event_vars_group = h5_file.get('EventVars')\n",
    "        if event_vars_group and 'normweight' in event_vars_group and len(event_vars_group['normweight']) == 0:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def copy_items(name, obj, destination_group, max_entries):\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        if name not in destination_group:\n",
    "            new_group = destination_group.create_group(name)\n",
    "            obj.visititems(lambda subname, subobj: copy_items(subname, subobj, new_group, max_entries))\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        if name in destination_group:\n",
    "            existing_dataset = destination_group[name]\n",
    "            existing_shape = existing_dataset.shape\n",
    "            new_shape = (existing_shape[0] + min(obj.shape[0], max_entries),) + existing_shape[1:]\n",
    "            existing_dataset.resize(new_shape)\n",
    "            existing_dataset[-min(obj.shape[0], max_entries):] = obj[:min(obj.shape[0], max_entries)]\n",
    "        else:\n",
    "            chunks = (1,) + obj.shape[1:]\n",
    "            new_shape = (min(obj.shape[0], max_entries),) + obj.shape[1:]\n",
    "            destination_group.create_dataset(name, shape=new_shape, dtype=obj.dtype, chunks=chunks, maxshape=(None,) + obj.shape[1:], data=obj[:min(obj.shape[0], max_entries)])\n",
    "\n",
    "def has_pt_above_threshold(pt_dataset, threshold, min_jets):\n",
    "    above_threshold_indices = pt_dataset[:] > threshold\n",
    "    return above_threshold_indices.sum() >= min_jets\n",
    "\n",
    "def concatenate_h5_in_folder(output_file, folder='unsupervised-search/QCD_HT_h5s', threshold=2, min_jets=4, max_entries=100000):\n",
    "    files_to_concatenate = glob.glob(os.path.join(folder, '*.h5'))\n",
    "\n",
    "    with h5py.File(output_file, 'w') as output_h5:\n",
    "        for file_path in files_to_concatenate:\n",
    "            print(file_path)\n",
    "            with h5py.File(file_path, 'r') as input_h5:\n",
    "                input_pt_dataset = input_h5.get('source/pt')\n",
    "                if input_pt_dataset is not None and has_pt_above_threshold(input_pt_dataset, threshold, min_jets):\n",
    "                    input_h5.visititems(lambda name, obj: copy_items(name, obj, output_h5, max_entries))\n",
    "\n",
    "output_file_path = 'unsupervised-search/combined_QCD_400k.h5'\n",
    "concatenate_h5_in_folder(output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multijets_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
